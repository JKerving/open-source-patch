diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java
index 1dcc196..253d196 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java
@@ -165,4 +165,9 @@ HdfsBlocksMetadata getHdfsBlocksMetadata(String blockPoolId,
    */
   void triggerBlockReport(BlockReportOptions options)
     throws IOException;
+
+  /**
+   * Refresh the data xceiver count value
+   */
+  void refreshXceiverCount() throws IOException;
 }
diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolServerSideTranslatorPB.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolServerSideTranslatorPB.java
index 5c2c4a7..16a8cf2 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolServerSideTranslatorPB.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolServerSideTranslatorPB.java
@@ -23,6 +23,7 @@
 import java.util.Map;
 
 import com.google.common.base.Optional;
+
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.ReconfigurationTaskStatus;
 import org.apache.hadoop.conf.ReconfigurationUtil.PropertyChange;
@@ -46,6 +47,8 @@
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.GetReplicaVisibleLengthResponseProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.RefreshNamenodesRequestProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.RefreshNamenodesResponseProto;
+import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.RefreshXceiverCountRequestProto;
+import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.RefreshXceiverCountResponseProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.ShutdownDatanodeRequestProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.ShutdownDatanodeResponseProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.StartReconfigurationRequestProto;
@@ -79,6 +82,8 @@
       StartReconfigurationResponseProto.newBuilder().build();
   private final static TriggerBlockReportResponseProto TRIGGER_BLOCK_REPORT_RESP =
       TriggerBlockReportResponseProto.newBuilder().build();
+  private final static RefreshXceiverCountResponseProto REFRESH_XCEIVER_COUNT_RESP =
+      RefreshXceiverCountResponseProto.newBuilder().build();
   
   private final ClientDatanodeProtocol impl;
 
@@ -255,4 +260,16 @@ public TriggerBlockReportResponseProto triggerBlockReport(
     }
     return TRIGGER_BLOCK_REPORT_RESP;
   }
+
+  @Override
+  public RefreshXceiverCountResponseProto refreshXceiverCount(
+      RpcController controller, RefreshXceiverCountRequestProto request)
+      throws ServiceException {
+    try {
+      impl.refreshXceiverCount();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+    return REFRESH_XCEIVER_COUNT_RESP;
+  }
 }
diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolTranslatorPB.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolTranslatorPB.java
index f1a1b24..ff16751 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolTranslatorPB.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolTranslatorPB.java
@@ -28,6 +28,7 @@
 
 import com.google.common.base.Optional;
 import com.google.common.collect.Maps;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
@@ -56,6 +57,7 @@
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.GetReconfigurationStatusRequestProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.GetReconfigurationStatusResponseProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.GetReconfigurationStatusConfigChangeProto;
+import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.RefreshXceiverCountRequestProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.ShutdownDatanodeRequestProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.StartReconfigurationRequestProto;
 import org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos.TriggerBlockReportRequestProto;
@@ -349,4 +351,14 @@ public void triggerBlockReport(BlockReportOptions options)
       throw ProtobufHelper.getRemoteException(e);
     }
   }
+
+  @Override
+  public void refreshXceiverCount() throws IOException {
+    try {
+      rpcProxy.refreshXceiverCount(NULL_CONTROLLER,
+          RefreshXceiverCountRequestProto.newBuilder().build());
+    } catch (ServiceException e) {
+      throw ProtobufHelper.getRemoteException(e);
+    }
+  }
 }
diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
index 20cf0b4..3fee18b 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -3206,4 +3206,26 @@ public void removeSpanReceiver(long id) throws IOException {
     checkSuperuserPrivilege();
     spanReceiverHost.removeSpanReceiver(id);
   }
+
+  @Override
+  public void refreshXceiverCount() throws IOException {
+    int xceiverCount;
+    DataXceiverServer dxcs;
+
+    checkSuperuserPrivilege();
+    conf = new Configuration();
+    xceiverCount =
+        conf.getInt(DFSConfigKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_KEY,
+            DFSConfigKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_DEFAULT);
+
+    dxcs = (DataXceiverServer) this.dataXceiverServer.getRunnable();
+    dxcs.maxXceiverCount = xceiverCount;
+  }
+
+  @VisibleForTesting
+  public int getMaxXceiverCount() {
+    DataXceiverServer dxcs =
+        (DataXceiverServer) this.dataXceiverServer.getRunnable();
+    return dxcs.maxXceiverCount;
+  }
 }
diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java
index e80b4c0..2c3b261 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java
@@ -420,6 +420,8 @@ static int run(DistributedFileSystem dfs, String[] argv, int idx) throws IOExcep
     "\t[-getDatanodeInfo <datanode_host:ipc_port>]\n" +
     "\t[-metasave filename]\n" +
     "\t[-triggerBlockReport [-incremental] <datanode_host:ipc_port>]\n" +
+    "\t[-refreshXceiverCount " +
+    "[datanode_host1:ipc_port,datanode_host2:ipc_port]]\n" +
     "\t[-help [cmd]]\n";
 
   /**
@@ -1012,6 +1014,10 @@ private void printHelp(String cmd) {
         + "\tIf 'incremental' is specified, it will be an incremental\n"
         + "\tblock report; otherwise, it will be a full block report.\n";
 
+    String refreshXceiverCount =
+        "-refreshXceiverCount [datanode_host1:ipc_port,datanode_host2:ipc_port]\n"
+            + "\tReload the configuration and refresh the data xceiver count.\n";
+
     String help = "-help [cmd]: \tDisplays help for the given command or all commands if none\n" +
       "\t\tis specified.\n";
 
@@ -1104,6 +1110,7 @@ private void printHelp(String cmd) {
       System.out.println(shutdownDatanode);
       System.out.println(getDatanodeInfo);
       System.out.println(triggerBlockReport);
+      System.out.println(refreshXceiverCount);
       System.out.println(help);
       System.out.println();
       ToolRunner.printGenericCommandUsage(System.out);
@@ -1619,6 +1626,9 @@ private static void printUsage(String cmd) {
     } else if ("-triggerBlockReport".equals(cmd)) {
       System.err.println("Usage: hdfs dfsadmin"
           + " [-triggerBlockReport [-incremental] <datanode_host:ipc_port>]");
+    } else if ("-refreshXceiverCount".equals(cmd)) {
+      System.err.println("Usage: hdfs dfsadmin [-refreshXceiverCount "
+          + "[datanode_host1:ipc_port,datanode_host2:ipc_port,...]]");
     } else {
       System.err.println("Usage: hdfs dfsadmin");
       System.err.println("Note: Administrative commands can only be run as the HDFS superuser.");
@@ -1762,8 +1772,12 @@ public int run(String[] argv) throws Exception {
         printUsage(cmd);
         return exitCode;
       }
+    } else if ("-refreshXceiverCount".equals(cmd)) {
+      if (argv.length != 2) {
+        printUsage(cmd);
+        return exitCode;
+      }
     }
-    
     // initialize DFSAdmin
     try {
       init();
@@ -1837,6 +1851,8 @@ public int run(String[] argv) throws Exception {
         exitCode = reconfig(argv, i);
       } else if ("-triggerBlockReport".equals(cmd)) {
         exitCode = triggerBlockReport(argv);
+      } else if ("-refreshXceiverCount".equals(cmd)) {
+        exitCode = refreshXceiverCount(argv, i);
       } else if ("-help".equals(cmd)) {
         if (i < argv.length) {
           printHelp(argv[i]);
@@ -1921,6 +1937,31 @@ private int refreshNamenodes(String[] argv, int i) throws IOException {
     return 0;
   }
 
+  private int refreshXceiverCount(String[] argv, int i) throws IOException {
+    int exitCode = -1;
+    String[] datanodeHosts;
+    ClientDatanodeProtocol dnProxy;
+
+    exitCode = 0;
+    datanodeHosts = argv[i].split(",");
+
+    if (datanodeHosts != null) {
+      for (String dnHost : datanodeHosts) {
+        try {
+          dnProxy = getDataNodeProxy(dnHost);
+          dnProxy.refreshXceiverCount();
+
+          System.out.println("Refresh data xceiver count for node " + dnHost);
+        } catch (IOException ioe) {
+          System.err.println("Datanode " + dnHost + " unreachable.");
+          return -1;
+        }
+      }
+    }
+
+    return exitCode;
+  }
+
   private int shutdownDatanode(String[] argv, int i) throws IOException {
     final String dn = argv[i];
     ClientDatanodeProtocol dnProxy = getDataNodeProxy(dn);
diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/proto/ClientDatanodeProtocol.proto hadoop-hdfs-project/hadoop-hdfs/src/main/proto/ClientDatanodeProtocol.proto
index 48f6dd1..6497014 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/proto/ClientDatanodeProtocol.proto
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/proto/ClientDatanodeProtocol.proto
@@ -163,6 +163,12 @@ message TriggerBlockReportRequestProto {
 message TriggerBlockReportResponseProto {
 }
 
+message RefreshXceiverCountRequestProto {
+}
+
+message RefreshXceiverCountResponseProto {
+}
+
 /** Query the running status of reconfiguration process */
 message GetReconfigurationStatusRequestProto {
 }
@@ -232,4 +238,7 @@ service ClientDatanodeProtocolService {
 
   rpc triggerBlockReport(TriggerBlockReportRequestProto)
       returns(TriggerBlockReportResponseProto);
+
+  rpc refreshXceiverCount(RefreshXceiverCountRequestProto)
+      returns(RefreshXceiverCountResponseProto);
 }
diff --git hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdmin.java hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdmin.java
index 9758955..5afc6c9 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdmin.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdmin.java
@@ -20,8 +20,10 @@
 import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY;
 
 import com.google.common.collect.Lists;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.ReconfigurationUtil;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hdfs.server.common.Storage;
 import org.apache.hadoop.hdfs.server.datanode.DataNode;
@@ -42,6 +44,7 @@
 import static org.hamcrest.CoreMatchers.anyOf;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.CoreMatchers.not;
+import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertThat;
 import static org.junit.Assert.assertTrue;
 import static org.hamcrest.CoreMatchers.containsString;
@@ -148,4 +151,47 @@ public void testGetReconfigureStatus()
     assertThat(outputs.get(failedOffset + 2),
         containsString("To: \"new123\""));
   }
+
+  @Test(timeout = 30000)
+  public void testRefreshXceiverCount() throws Exception {
+    int DEFAULT_DATA_XCEIVER_COUNT = 1024;
+    int NUM_OF_DATANODES = 2;
+
+    Configuration conf = new Configuration();
+    /* Set xceiverCount to a low default value . */
+    conf.setLong(DFSConfigKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_KEY,
+        DEFAULT_DATA_XCEIVER_COUNT);
+
+    /* Create and start cluster */
+    cluster =
+        new MiniDFSCluster.Builder(conf).numDataNodes(NUM_OF_DATANODES).build();
+    DFSAdmin admin = new DFSAdmin(conf);
+
+    try {
+      cluster.waitActive();
+      ArrayList<DataNode> datanodes = cluster.getDataNodes();
+
+      // Ensure value from the configuration is reflected in the datanodes.
+      assertEquals(DEFAULT_DATA_XCEIVER_COUNT, (long) datanodes.get(0)
+          .getMaxXceiverCount());
+      assertEquals(DEFAULT_DATA_XCEIVER_COUNT, (long) datanodes.get(1)
+          .getMaxXceiverCount());
+
+      String dn1Address =
+          datanodes.get(0).getDatanodeId().getIpAddr() + ":"
+              + datanodes.get(0).getIpcPort();
+      // new xceiverCount value
+      long newXceiverCount =
+          DFSConfigKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_DEFAULT;
+      String[] args = { "-refreshXceiverCount", dn1Address };
+      assertEquals(0, admin.run(args));
+
+      assertEquals(newXceiverCount, (long) datanodes.get(0)
+          .getMaxXceiverCount());
+      assertEquals(DEFAULT_DATA_XCEIVER_COUNT, (long) datanodes.get(1)
+          .getMaxXceiverCount());
+    } finally {
+      cluster.shutdown();
+    }
+  }
 }
\ No newline at end of file
