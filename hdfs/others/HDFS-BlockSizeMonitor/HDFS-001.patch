diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
index 018461d..4684547 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
@@ -389,6 +389,7 @@
   public static final String  DFS_CLIENT_RETRY_WINDOW_BASE= "dfs.client.retry.window.base";
   public static final String  DFS_METRICS_SESSION_ID_KEY = "dfs.metrics.session-id";
   public static final String  DFS_METRICS_PERCENTILES_INTERVALS_KEY = "dfs.metrics.percentiles.intervals";
+  public static final String  DFS_METRICS_SPILT_BLOCK_SIZE_KEY = "dfs.metrics.spilt-block.size";
   public static final String  DFS_DATANODE_HOST_NAME_KEY = "dfs.datanode.hostname";
   public static final String  DFS_NAMENODE_HOSTS_KEY = "dfs.namenode.hosts";
   public static final String  DFS_NAMENODE_HOSTS_EXCLUDE_KEY = "dfs.namenode.hosts.exclude";
diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
index eceea64..0a0692f 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
@@ -1260,6 +1260,52 @@ public static int roundBytesToGB(long bytes) {
     return Math.round((float)bytes/ 1024 / 1024 / 1024);
   }
   
+  /**
+   * Round bytes to MB
+   *
+   * @param bytes number of bytes
+   * @return number of MB
+   */
+  public static int roundBytesToMB(long bytes) {
+    return Math.round((float) bytes / 1024 / 1024);
+  }
+
+  /**
+   * Round bytes to KB
+   *
+   * @param bytes number of bytes
+   * @return number of KB
+   */
+  public static int roundBytesToKB(long bytes) {
+    return Math.round((float) bytes / 1024);
+  }
+
+  public static String roundBytesToStandardFormat(long bytes) {
+    int value;
+    String format;
+    String suffix;
+
+    format = "";
+    suffix = "";
+    value = -1;
+    if (bytes / (1024 * 1024 * 1024) > 0) {
+      value = roundBytesToGB(bytes);
+      suffix = "G";
+    } else if (bytes / (1024 * 1024) > 0) {
+      value = roundBytesToMB(bytes);
+      suffix = "M";
+    } else if (bytes / 1024 > 0) {
+      value = roundBytesToKB(bytes);
+      suffix = "K";
+    } else {
+      value = (int) bytes;
+      suffix = "B";
+    }
+
+    format = String.valueOf(value) + suffix;
+    return format;
+  }
+
   /** Create a {@link ClientDatanodeProtocol} proxy */
   public static ClientDatanodeProtocol createClientDatanodeProtocolProxy(
       DatanodeID datanodeid, Configuration conf, int socketTimeout,
diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
index c057f3e..fdf3237 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
@@ -722,8 +722,10 @@ public LocatedBlock addBlock(String src, String clientName,
         : Arrays.asList(favoredNodes);
     LocatedBlock locatedBlock = namesystem.getAdditionalBlock(src, fileId,
         clientName, previous, excludedNodesSet, favoredNodesList);
-    if (locatedBlock != null)
+    if (locatedBlock != null) {
       metrics.incrAddBlockOps();
+      metrics.addBlockSize(locatedBlock.getBlockSize());
+    }
     return locatedBlock;
   }
 
diff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java
index 4394f9d..ed558f8 100644
--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java
+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java
@@ -22,7 +22,9 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.DFSUtil;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl;
 import org.apache.hadoop.metrics2.MetricsSystem;
 import org.apache.hadoop.metrics2.annotation.Metric;
 import org.apache.hadoop.metrics2.annotation.Metrics;
@@ -33,6 +35,9 @@
 import org.apache.hadoop.metrics2.lib.MutableQuantiles;
 import org.apache.hadoop.metrics2.lib.MutableRate;
 import org.apache.hadoop.metrics2.source.JvmMetrics;
+import org.mortbay.log.Log;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * This class is for maintaining  the various NameNode activity statistics
@@ -40,6 +45,8 @@
  */
 @Metrics(name="NameNodeActivity", about="NameNode metrics", context="dfs")
 public class NameNodeMetrics {
+  public static final Logger LOG = LoggerFactory
+      .getLogger(NameNodeMetrics.class);
   final MetricsRegistry registry = new MetricsRegistry("namenode");
 
   @Metric MutableCounterLong createFileOps;
@@ -129,9 +136,14 @@ public long totalFileOps(){
   MutableRate putImage;
 
   JvmMetrics jvmMetrics = null;
-  
+
+  MutableRate[] blockSpilts;
+
+  private final static int BLOCK_SPILT_SIZE = 4;
+  private long unitSpiltBytes = 0;
+
   NameNodeMetrics(String processName, String sessionId, int[] intervals,
-      final JvmMetrics jvmMetrics) {
+      int spiltNum, long blockSize, final JvmMetrics jvmMetrics) {
     this.jvmMetrics = jvmMetrics;
     registry.tag(ProcessName, processName).tag(SessionId, sessionId);
     
@@ -152,6 +164,29 @@ public long totalFileOps(){
           "cacheReport" + interval + "s",
           "Cache report", "ops", "latency", interval);
     }
+
+    if (spiltNum <= 0) {
+      spiltNum = 1;
+    }
+    unitSpiltBytes = blockSize / spiltNum;
+    Log.info("blockSize: " + blockSize + ", spiltNum: " + spiltNum
+        + ", unitSpiltBytes: " + unitSpiltBytes);
+    blockSpilts = new MutableRate[spiltNum];
+    for (int i = 0; i < spiltNum; i++) {
+      long upperBoundSize;
+      long lowerBoundSize = i * unitSpiltBytes;
+
+      if (i < spiltNum - 1) {
+        upperBoundSize = (i + 1) * unitSpiltBytes;
+      } else {
+        upperBoundSize = blockSize;
+      }
+
+      blockSpilts[i] =
+          registry.newRate("CreateBlockSizeFrom"
+              + DFSUtil.roundBytesToStandardFormat(lowerBoundSize) + "To"
+              + DFSUtil.roundBytesToStandardFormat(upperBoundSize));
+    }
   }
 
   public static NameNodeMetrics create(Configuration conf, NamenodeRole r) {
@@ -159,12 +194,18 @@ public static NameNodeMetrics create(Configuration conf, NamenodeRole r) {
     String processName = r.toString();
     MetricsSystem ms = DefaultMetricsSystem.instance();
     JvmMetrics jm = JvmMetrics.create(processName, sessionId, ms);
-    
+
+    int spiltNum =
+        conf.getInt(DFSConfigKeys.DFS_METRICS_SPILT_BLOCK_SIZE_KEY,
+            BLOCK_SPILT_SIZE);
+    long blockSize =
+        conf.getLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,
+            DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT);
     // Percentile measurement is off by default, by watching no intervals
     int[] intervals = 
         conf.getInts(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY);
     return ms.register(new NameNodeMetrics(processName, sessionId,
-        intervals, jm));
+        intervals, spiltNum, blockSize, jm));
   }
 
   public JvmMetrics getJvmMetrics() {
@@ -323,4 +364,11 @@ public void addPutImage(long latency) {
   public void setHasDiskSpaceForEditlog(int hasResource){
     hasDiskSpaceForEditlog.set(hasResource);
   }
+
+  public void addBlockSize(long blockBytes) {
+    int spiltIndex;
+
+    spiltIndex = (int) (blockBytes / unitSpiltBytes);
+    blockSpilts[spiltIndex].add(blockBytes);
+  }
 }
