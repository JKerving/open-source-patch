diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
index 2bd28ab..637aae5 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
@@ -187,7 +187,9 @@
       + "\n\t[-runDuringUpgrade]"
       + "\tWhether to run the balancer during an ongoing HDFS upgrade."
       + "This is usually not desired since it will not affect used space "
-      + "on over-utilized machines.";
+      + "on over-utilized machines."
+      + "\n\t[-blockbytesnum <blockbytesnum>]\tThe minimum limitation of "
+      + "bytes size that source block should larger than this value.";
 
   private final Dispatcher dispatcher;
   private final NameNodeConnector nnc;
@@ -272,7 +274,7 @@ static int getInt(Configuration conf, String key, int defaultValue) {
         new Dispatcher(theblockpool, p.getIncludedNodes(),
             p.getExcludedNodes(), movedWinWidth, moverThreads,
             dispatcherThreads, maxConcurrentMovesPerNode, getBlocksSize,
-            getBlocksMinBlockSize, conf);
+            getBlocksMinBlockSize, p.getBlockBytesNum(), conf);
     this.threshold = p.getThreshold();
     this.policy = p.getBalancingPolicy();
     this.sourceNodes = p.getSourceNodes();
@@ -811,6 +813,24 @@ static BalancerParameters parse(String[] args) {
                   + "upgrade. Most users will not want to run the balancer "
                   + "during an upgrade since it will not affect used space "
                   + "on over-utilized machines.");
+            } else if ("-blockBytesNum".equalsIgnoreCase(args[i])) {
+              checkArgument(
+                  ++i < args.length,
+                  "blockBytesNum value is missing: args = "
+                      + Arrays.toString(args));
+              try {
+                long blockBytesNum = Long.parseLong(args[i]);
+                if (blockBytesNum <= 0) {
+                  throw new IllegalArgumentException(
+                      "Number out of range: blockbytesNum = " + blockBytesNum);
+                }
+                LOG.info("Using a blockBytesNum of " + blockBytesNum);
+                b.setBlockBytesNum(blockBytesNum);
+              } catch (IllegalArgumentException e) {
+                System.err.println("Expecting a number larger or equal to 1");
+                throw e;
+              }
+
             } else {
               throw new IllegalArgumentException("args = "
                   + Arrays.toString(args));
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/BalancerParameters.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/BalancerParameters.java
index 5d5e9b1..4dfd9ac 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/BalancerParameters.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/BalancerParameters.java
@@ -44,6 +44,11 @@
    * Whether to run the balancer during upgrade.
    */
   private final boolean runDuringUpgrade;
+  
+  /**
+   * limit source block size
+   */
+  private final long blockBytesNum;
 
   static final BalancerParameters DEFAULT = new BalancerParameters();
 
@@ -60,6 +65,7 @@ private BalancerParameters(Builder builder) {
     this.sourceNodes = builder.sourceNodes;
     this.blockpools = builder.blockpools;
     this.runDuringUpgrade = builder.runDuringUpgrade;
+    this.blockBytesNum = builder.blockBytesNum;
   }
 
   BalancingPolicy getBalancingPolicy() {
@@ -94,16 +100,21 @@ boolean getRunDuringUpgrade() {
     return this.runDuringUpgrade;
   }
 
+  long getBlockBytesNum() {
+    return this.blockBytesNum;
+  }
+
   @Override
   public String toString() {
     return String.format("%s.%s [%s," + " threshold = %s,"
         + " max idle iteration = %s," + " #excluded nodes = %s,"
         + " #included nodes = %s," + " #source nodes = %s,"
-        + " #blockpools = %s," + " run during upgrade = %s]",
+        + " #blockpools = %s," + " run during upgrade = %s]"
+        + " #blockbytesnum = %s]",
         Balancer.class.getSimpleName(), getClass().getSimpleName(), policy,
         threshold, maxIdleIteration, excludedNodes.size(),
         includedNodes.size(), sourceNodes.size(), blockpools.size(),
-        runDuringUpgrade);
+        runDuringUpgrade, blockBytesNum);
   }
 
   static class Builder {
@@ -117,6 +128,7 @@ public String toString() {
     private Set<String> sourceNodes = Collections.<String> emptySet();
     private Set<String> blockpools = Collections.<String> emptySet();
     private boolean runDuringUpgrade = false;
+    private long blockBytesNum = 1;
 
     Builder() {
     }
@@ -164,5 +176,10 @@ Builder setRunDuringUpgrade(boolean run) {
     BalancerParameters build() {
       return new BalancerParameters(this);
     }
+
+    Builder setBlockBytesNum(long bytes) {
+      this.blockBytesNum = bytes;
+      return this;
+    }
   }
 }
\ No newline at end of file
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java
index 5b3eb36..6463bec 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java
@@ -120,6 +120,8 @@
 
   private final long getBlocksSize;
   private final long getBlocksMinBlockSize;
+  /** source block size limit value */
+  private final long blockBytesNum;
 
   private final int ioFileBufferSize;
 
@@ -920,13 +922,13 @@ public Dispatcher(NameNodeConnector nnc, Set<String> includedNodes,
       int dispatcherThreads, int maxConcurrentMovesPerNode, Configuration conf) {
     this(nnc, includedNodes, excludedNodes, movedWinWidth,
         moverThreads, dispatcherThreads, maxConcurrentMovesPerNode,
-        0L, 0L, conf);
+        0L, 0L, 1L, conf);
   }
 
   Dispatcher(NameNodeConnector nnc, Set<String> includedNodes,
       Set<String> excludedNodes, long movedWinWidth, int moverThreads,
-      int dispatcherThreads, int maxConcurrentMovesPerNode,
-      long getBlocksSize, long getBlocksMinBlockSize, Configuration conf) {
+      int dispatcherThreads, int maxConcurrentMovesPerNode, long getBlocksSize,
+      long getBlocksMinBlockSize, long blockBytesNum, Configuration conf) {
     this.nnc = nnc;
     this.excludedNodes = excludedNodes;
     this.includedNodes = includedNodes;
@@ -941,6 +943,7 @@ public Dispatcher(NameNodeConnector nnc, Set<String> includedNodes,
 
     this.getBlocksSize = getBlocksSize;
     this.getBlocksMinBlockSize = getBlocksMinBlockSize;
+    this.blockBytesNum = blockBytesNum;
 
     this.saslClient = new SaslDataTransferClient(conf,
         DataTransferSaslUtil.getSaslPropertiesResolver(conf),
@@ -1173,6 +1176,12 @@ private boolean isGoodBlockCandidate(StorageGroup source, StorageGroup target,
     if (reduceNumOfRacks(source, target, block)) {
       return false;
     }
+
+    // check if block size is larger than block bytes num
+    if (block.getNumBytes() < this.blockBytesNum) {
+      return false;
+    }
+
     return true;
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
index 332ae15..b53333e 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
@@ -1451,6 +1451,86 @@ public void testBalancerDuringUpgrade() throws Exception {
   }
 
   /**
+   * Test balancer with blockBytesNum. The blockBytesNum will filter small
+   * blocks. We have 2 blockBytesNum value.One is smaller and the other is
+   * larger. And if the file block size if smaller than blockBytesNum'value the
+   * block will be no move.
+   */
+  @Test(timeout = 100000)
+  public void testBalancerWithBlockBytesNum() throws Exception {
+    final Configuration conf = new HdfsConfiguration();
+
+    int blockSize = 5 * 1024 * 1024;
+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, blockSize);
+    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1L);
+    conf.setLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, 1L);
+
+    int writeDatanodes = 1;
+    final MiniDFSCluster cluster =
+        new MiniDFSCluster.Builder(conf)
+            .numDataNodes(2)
+            .racks(new String[] { "/default/rack0", "/default/rack0" })
+            .storagesPerDatanode(2)
+            .storageTypes(
+                new StorageType[][] { { StorageType.SSD, StorageType.DISK },
+                    { StorageType.SSD, StorageType.DISK } })
+            .storageCapacities(
+                new long[][] { { 100 * blockSize, 20 * blockSize },
+                    { 20 * blockSize, 100 * blockSize } }).build();
+
+    try {
+      cluster.waitActive();
+
+      // set "/bar" directory with ONE_SSD storage policy.
+      DistributedFileSystem fs = cluster.getFileSystem();
+      Path barDir = new Path("/bar");
+      fs.mkdir(barDir, new FsPermission((short) 777));
+      fs.setStoragePolicy(barDir, HdfsConstants.ONESSD_STORAGE_POLICY_NAME);
+
+      // Insert 30 blocks to one datanode
+      long fileLen = 30 * blockSize;
+      // create file named foo
+      Path fooFile = new Path(barDir, "foo");
+      createFile(cluster, fooFile, fileLen, (short) writeDatanodes, 0);
+      // update space info
+      cluster.triggerHeartbeats();
+
+      int r;
+      // Run balancer
+      BalancerParameters p;
+      BalancerParameters.Builder b;
+      Collection<URI> namenodes;
+      // set block limit value is 10M more than blockSize above so
+      // that should be no move
+      int blockBytesNum = 10 * 1024 * 1024;
+
+      b = new BalancerParameters.Builder();
+      b.setBlockBytesNum(blockBytesNum);
+      p = b.build();
+
+      namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      r = Balancer.run(namenodes, p, conf);
+
+      assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(), r);
+
+      // update block bytes num smaller as 1M
+      blockBytesNum = 1 * 1024 * 1024;
+      b = new BalancerParameters.Builder();
+      b.setBlockBytesNum(blockBytesNum);
+      p = b.build();
+
+      namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      r = Balancer.run(namenodes, p, conf);
+
+      // the limit value is smaller than block size so the block will move
+      assertEquals(ExitStatus.SUCCESS.getExitCode(), r);
+
+    } finally {
+      cluster.shutdown();
+    }
+  }
+
+  /**
    * Test special case. Two replicas belong to same block should not in same node.
    * We have 2 nodes.
    * We have a block in (DN0,SSD) and (DN1,DISK).
