diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
index b10a719..34fb8fb 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java
@@ -57,6 +57,7 @@
 import org.apache.hadoop.util.Time;
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
+
 import static com.google.common.base.Preconditions.checkArgument;
 
 import com.google.common.base.Preconditions;
@@ -181,7 +182,9 @@
       + "\n\t[-include [-f <hosts-file> | <comma-separated list of hosts>]]"
       + "\tIncludes only the specified datanodes."
       + "\n\t[-idleiterations <idleiterations>]"
-      + "\tNumber of consecutive idle iterations (-1 for Infinite) before exit.";
+      + "\tNumber of consecutive idle iterations (-1 for Infinite) before exit."
+      + "\n\t[-blockbytesnum <blockbytesnum>]\tThe minimum limitation of "
+      + "bytes size that source block should larger than this value.";
   
   private final Dispatcher dispatcher;
   private final BalancingPolicy policy;
@@ -228,9 +231,10 @@ private static void checkReplicationPolicyCompatibility(Configuration conf
         DFSConfigKeys.DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY,
         DFSConfigKeys.DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_DEFAULT);
 
-    this.dispatcher = new Dispatcher(theblockpool, p.nodesToBeIncluded,
-        p.nodesToBeExcluded, movedWinWidth, moverThreads, dispatcherThreads,
-        maxConcurrentMovesPerNode, conf);
+    this.dispatcher =
+        new Dispatcher(theblockpool, p.blockBytesNum, p.nodesToBeIncluded,
+            p.nodesToBeExcluded, movedWinWidth, moverThreads,
+            dispatcherThreads, maxConcurrentMovesPerNode, conf);
     this.threshold = p.threshold;
     this.policy = p.policy;
   }
@@ -634,22 +638,25 @@ private static String time2Str(long elapsedTime) {
   static class Parameters {
     static final Parameters DEFAULT = new Parameters(
         BalancingPolicy.Node.INSTANCE, 10.0,
-        NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS,
+        NameNodeConnector.DEFAULT_MAX_IDLE_ITERATIONS, 1,
         Collections.<String> emptySet(), Collections.<String> emptySet());
 
     final BalancingPolicy policy;
     final double threshold;
     final int maxIdleIteration;
+    final long blockBytesNum;
     // exclude the nodes in this set from balancing operations
     Set<String> nodesToBeExcluded;
     //include only these nodes in balancing operations
     Set<String> nodesToBeIncluded;
 
     Parameters(BalancingPolicy policy, double threshold, int maxIdleIteration,
-        Set<String> nodesToBeExcluded, Set<String> nodesToBeIncluded) {
+        long blockBytesNum, Set<String> nodesToBeExcluded,
+        Set<String> nodesToBeIncluded) {
       this.policy = policy;
       this.threshold = threshold;
       this.maxIdleIteration = maxIdleIteration;
+      this.blockBytesNum = blockBytesNum;
       this.nodesToBeExcluded = nodesToBeExcluded;
       this.nodesToBeIncluded = nodesToBeIncluded;
     }
@@ -659,6 +666,7 @@ public String toString() {
       return Balancer.class.getSimpleName() + "." + getClass().getSimpleName()
           + "[" + policy + ", threshold=" + threshold +
           ", max idle iteration = " + maxIdleIteration +
+          ", block bytes num = " + blockBytesNum +
           ", number of nodes to be excluded = "+ nodesToBeExcluded.size() +
           ", number of nodes to be included = "+ nodesToBeIncluded.size() +"]";
     }
@@ -700,6 +708,7 @@ static Parameters parse(String[] args) {
       BalancingPolicy policy = Parameters.DEFAULT.policy;
       double threshold = Parameters.DEFAULT.threshold;
       int maxIdleIteration = Parameters.DEFAULT.maxIdleIteration;
+      long blockBytesNum = Parameters.DEFAULT.blockBytesNum;
       Set<String> nodesTobeExcluded = Parameters.DEFAULT.nodesToBeExcluded;
       Set<String> nodesTobeIncluded = Parameters.DEFAULT.nodesToBeIncluded;
 
@@ -760,6 +769,23 @@ static Parameters parse(String[] args) {
                   "idleiterations value is missing: args = " + Arrays.toString(args));
               maxIdleIteration = Integer.parseInt(args[i]);
               LOG.info("Using a idleiterations of " + maxIdleIteration);
+            } else if ("-blockBytesNum".equalsIgnoreCase(args[i])) {
+              checkArgument(
+                  ++i < args.length,
+                  "blockBytesNum value is missing: args = "
+                      + Arrays.toString(args));
+              try {
+                blockBytesNum = Long.parseLong(args[i]);
+                if (blockBytesNum <= 0) {
+                  throw new IllegalArgumentException(
+                      "Number out of range: blockbytesNum = " + threshold);
+                }
+                LOG.info("Using a blockBytesNum of " + blockBytesNum);
+              } catch(IllegalArgumentException e) {
+                System.err.println("Expecting a number larger or equal to 1");
+                throw e;
+              }
+
             } else {
               throw new IllegalArgumentException("args = "
                   + Arrays.toString(args));
@@ -773,7 +799,7 @@ static Parameters parse(String[] args) {
         }
       }
       
-      return new Parameters(policy, threshold, maxIdleIteration, nodesTobeExcluded, nodesTobeIncluded);
+      return new Parameters(policy, threshold, maxIdleIteration, blockBytesNum, nodesTobeExcluded, nodesTobeIncluded);
     }
 
     private static void printUsage(PrintStream out) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java
index 144cf9d..bac0064 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java
@@ -98,6 +98,7 @@
   private final NameNodeConnector nnc;
   private final SaslDataTransferClient saslClient;
 
+  private final long blockBytesNum;
   /** Set of datanodes to be excluded. */
   private final Set<String> excludedNodes;
   /** Restrict to the following nodes. */
@@ -786,10 +787,12 @@ public boolean equals(Object obj) {
     }
   }
 
-  public Dispatcher(NameNodeConnector nnc, Set<String> includedNodes,
-      Set<String> excludedNodes, long movedWinWidth, int moverThreads,
-      int dispatcherThreads, int maxConcurrentMovesPerNode, Configuration conf) {
+  public Dispatcher(NameNodeConnector nnc, long blockBytesNum,
+      Set<String> includedNodes, Set<String> excludedNodes, long movedWinWidth,
+      int moverThreads, int dispatcherThreads, int maxConcurrentMovesPerNode,
+      Configuration conf) {
     this.nnc = nnc;
+    this.blockBytesNum = blockBytesNum;
     this.excludedNodes = excludedNodes;
     this.includedNodes = includedNodes;
     this.movedBlocks = new MovedBlocks<StorageGroup>(movedWinWidth);
@@ -1003,6 +1006,12 @@ private boolean isGoodBlockCandidate(StorageGroup source, StorageGroup target,
     if (reduceNumOfRacks(source, target, block)) {
       return false;
     }
+
+    // check if block size is larger than block bytes num
+    if (block.getNumBytes() < this.blockBytesNum) {
+      return false;
+    }
+
     return true;
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java
index 466b91f..98f666f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java
@@ -128,7 +128,7 @@ private StorageGroup getTarget(String uuid, StorageType storageType) {
         DFSConfigKeys.DFS_MOVER_RETRY_MAX_ATTEMPTS_KEY,
         DFSConfigKeys.DFS_MOVER_RETRY_MAX_ATTEMPTS_DEFAULT);
     this.retryCount = retryCount;
-    this.dispatcher = new Dispatcher(nnc, Collections.<String> emptySet(),
+    this.dispatcher = new Dispatcher(nnc, 1, Collections.<String> emptySet(),
         Collections.<String> emptySet(), movedWinWidth, moverThreads, 0,
         maxConcurrentMovesPerNode, conf);
     this.storages = new StorageMap();
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
index 5ee73d4..3cbddda 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
@@ -623,6 +623,7 @@ private void doTest(Configuration conf, long[] capacities,
             Balancer.Parameters.DEFAULT.policy,
             Balancer.Parameters.DEFAULT.threshold,
             Balancer.Parameters.DEFAULT.maxIdleIteration,
+            Balancer.Parameters.DEFAULT.blockBytesNum,
             nodes.getNodesToBeExcluded(), nodes.getNodesToBeIncluded());
       }
 
@@ -862,6 +863,7 @@ public void testUnknownDatanode() throws Exception {
           Balancer.Parameters.DEFAULT.policy,
           Balancer.Parameters.DEFAULT.threshold,
           Balancer.Parameters.DEFAULT.maxIdleIteration,
+          Balancer.Parameters.DEFAULT.blockBytesNum,
           datanodes, Balancer.Parameters.DEFAULT.nodesToBeIncluded);
       final int r = Balancer.run(namenodes, p, conf);
       assertEquals(ExitStatus.SUCCESS.getExitCode(), r);
@@ -1057,6 +1059,14 @@ public void testBalancerCliParseWithWrongParams() {
     } catch (IllegalArgumentException e) {
 
     }
+    
+    parameters = new String[] { "-blockBytesNum" };
+    try {
+      Balancer.Cli.parse(parameters);
+      fail(reason);
+    } catch (IllegalArgumentException e) {
+
+    }
   }
 
 
@@ -1297,6 +1307,7 @@ public void testBalancerWithRamDisk() throws Exception {
         Parameters.DEFAULT.policy,
         Parameters.DEFAULT.threshold,
         Balancer.Parameters.DEFAULT.maxIdleIteration,
+        Parameters.DEFAULT.blockBytesNum,
         Parameters.DEFAULT.nodesToBeExcluded,
         Parameters.DEFAULT.nodesToBeIncluded);
       final int r = Balancer.run(namenodes, p, conf);
@@ -1313,6 +1324,91 @@ public void testBalancerWithRamDisk() throws Exception {
   }
 
   /**
+   * Test balancer with blockBytesNum. The blockBytesNum will filter small
+   * blocks. We have 2 blockBytesNum value.One is smaller and the other is
+   * larger. And if the file block size if smaller than blockBytesNum'value the
+   * block will be no move.
+   */
+  @Test(timeout=100000)
+  public void testBalancerWithBlockBytesNum() throws Exception {
+    final Configuration conf = new HdfsConfiguration();
+
+    int blockSize = 5 * 1024 * 1024 ;
+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, blockSize);
+    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1L);
+    conf.setLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, 1L);
+
+    int writeDatanodes =1;
+    final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)
+        .numDataNodes(2)
+        .racks(new String[]{"/default/rack0", "/default/rack0"})
+        .storagesPerDatanode(2)
+        .storageTypes(new StorageType[][]{
+            {StorageType.SSD, StorageType.DISK},
+            {StorageType.SSD, StorageType.DISK}})
+        .storageCapacities(new long[][]{
+            {100 * blockSize, 20 * blockSize},
+            {20 * blockSize, 100 * blockSize}})
+        .build();
+
+    try {
+      cluster.waitActive();
+
+      //set "/bar" directory with ONE_SSD storage policy.
+      DistributedFileSystem fs = cluster.getFileSystem();
+      Path barDir = new Path("/bar");
+      fs.mkdir(barDir,new FsPermission((short)777));
+      fs.setStoragePolicy(barDir, HdfsConstants.ONESSD_STORAGE_POLICY_NAME);
+
+      // Insert 30 blocks to one datanode
+      long fileLen  = 30 * blockSize;
+      // create file named foo
+      Path fooFile = new Path(barDir, "foo");
+      createFile(cluster, fooFile, fileLen, (short) writeDatanodes, 0);
+      // update space info
+      cluster.triggerHeartbeats();
+
+      int r;
+      Balancer.Parameters p;
+      Collection<URI> namenodes;
+      // set block limit value is 10M more than blockSize above so
+      // that should be no move
+      int blockBytesNum = 10 * 1024 * 1024;
+
+      p = new Balancer.Parameters(
+          Balancer.Parameters.DEFAULT.policy,
+          Balancer.Parameters.DEFAULT.threshold,
+          Balancer.Parameters.DEFAULT.maxIdleIteration,
+          blockBytesNum,
+          Balancer.Parameters.DEFAULT.nodesToBeExcluded,
+          Balancer.Parameters.DEFAULT.nodesToBeIncluded);
+
+      namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      r = Balancer.run(namenodes, p, conf);
+
+      assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(), r);
+
+      //update block bytes num smaller as 1M
+      blockBytesNum = 1 * 1024 * 1024;
+      p = new Balancer.Parameters(
+          Balancer.Parameters.DEFAULT.policy,
+          Balancer.Parameters.DEFAULT.threshold,
+          Balancer.Parameters.DEFAULT.maxIdleIteration,
+          blockBytesNum,
+          Balancer.Parameters.DEFAULT.nodesToBeExcluded,
+          Balancer.Parameters.DEFAULT.nodesToBeIncluded);
+      namenodes = DFSUtil.getNsServiceRpcUris(conf);
+      r = Balancer.run(namenodes, p, conf);
+
+      // the limit value is smaller than block size so the block will move
+      assertEquals(ExitStatus.SUCCESS.getExitCode(), r);
+
+    } finally {
+      cluster.shutdown();
+    }
+  }
+
+  /**
    * Test special case. Two replicas belong to same block should not in same node.
    * We have 2 nodes.
    * We have a block in (DN0,SSD) and (DN1,DISK).
